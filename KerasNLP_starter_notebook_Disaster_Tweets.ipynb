{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 17777,
          "databundleVersionId": 869809,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30527,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "KerasNLP starter notebook Disaster Tweets",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AWKohler/keras-comp/blob/main/KerasNLP_starter_notebook_Disaster_Tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "HF6sbmuWiV0N"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "nlp_getting_started_path = kagglehub.competition_download('nlp-getting-started')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "CurCJk3WiV0N"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://keras.io/img/logo-small.png\" alt=\"Keras logo\" width=\"100\"><br/>\n",
        "This starter notebook is provided by the Keras team.</center>\n",
        "\n",
        "## Keras NLP starter guide here: https://keras.io/guides/keras_nlp/getting_started/\n",
        "\n",
        "In this competition, the challenge is to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t.\n",
        "A dataset of 10,000 tweets that were hand classified is available.\n",
        "\n",
        "__This starter notebook uses the [DistilBERT](https://arxiv.org/abs/1910.01108) pretrained model from KerasNLP.__\n",
        "\n",
        "\n",
        "**BERT** stands for **Bidirectional Encoder Representations from Transformers**. BERT and other Transformer encoder architectures have been wildly successful on a variety of tasks in NLP (natural language processing). They compute vector-space representations of natural language that are suitable for use in deep learning models.\n",
        "\n",
        "The BERT family of models uses the **Transformer encoder architecture** to process each token of input text in the full context of all tokens before and after, hence the name: Bidirectional Encoder Representations from Transformers.\n",
        "\n",
        "BERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks.\n",
        "\n",
        "**DistilBERT model** is a distilled form of the **BERT** model. The size of a BERT model was reduced by 40% via knowledge distillation during the pre-training phase while retaining 97% of its language understanding abilities and being 60% faster.\n",
        "\n",
        "\n",
        "\n",
        "![BERT Architecture](https://www.cse.chalmers.se/~richajo/nlp2019/l5/bert_class.png)\n",
        "\n",
        "\n",
        "\n",
        "In this notebook, you will:\n",
        "\n",
        "- Load the Disaster Tweets\n",
        "- Explore the dataset\n",
        "- Preprocess the data\n",
        "- Load a DistilBERT model from Keras NLP\n",
        "- Train your own model, fine-tuning BERT\n",
        "- Generate the submission file\n"
      ],
      "metadata": {
        "id": "4ro4Cxw4iV0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install keras-core --upgrade\n",
        "# !pip install -q keras-nlp --upgrade\n",
        "\n",
        "# This sample uses Keras Core, the multi-backend version of Keras.\n",
        "# The selected backend is TensorFlow (other supported backends are 'jax' and 'torch')\n",
        "import os\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2025-04-09T03:14:26.14378Z",
          "iopub.execute_input": "2025-04-09T03:14:26.144245Z",
          "iopub.status.idle": "2025-04-09T03:14:26.150657Z",
          "shell.execute_reply.started": "2025-04-09T03:14:26.14421Z",
          "shell.execute_reply": "2025-04-09T03:14:26.148926Z"
        },
        "trusted": true,
        "id": "cTDfXQ9QiV0O"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import tensorflow as tf\n",
        "import keras_core as keras\n",
        "import keras_nlp\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"KerasNLP version:\", keras_nlp.__version__)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-04-09T03:14:28.796605Z",
          "iopub.execute_input": "2025-04-09T03:14:28.797877Z",
          "iopub.status.idle": "2025-04-09T03:14:30.062967Z",
          "shell.execute_reply.started": "2025-04-09T03:14:28.797836Z",
          "shell.execute_reply": "2025-04-09T03:14:30.061123Z"
        },
        "trusted": true,
        "id": "m0_SsJhciV0O",
        "outputId": "67586d10-672d-4722-fd58-0f13a59f56c6"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \u001b[38;5;66;03m# linear algebra\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m \u001b[38;5;66;03m# data processing, CSV file I/O (e.g. pd.read_csv)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras_core\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/__init__.py:51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/_api/v2/compat/__init__.py:37\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Compatibility functions.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mThe `tf.compat` module contains two sets of compatibility functions.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/_api/v2/compat/v1/__init__.py:31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py:37\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Compatibility functions.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mThe `tf.compat` module contains two sets of compatibility functions.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py:48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lite\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lookup\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/_api/v2/compat/v1/lite/__init__.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interpreter\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpHint\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/_api/v2/compat/v1/lite/experimental/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf.lite.experimental namespace.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m authoring\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalyzer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelAnalyzer \u001b[38;5;28;01mas\u001b[39;00m Analyzer\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpResolverType\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/_api/v2/compat/v1/lite/experimental/authoring/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf.lite.experimental.authoring namespace.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauthoring\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauthoring\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compatible\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/lite/python/authoring/authoring.py:43\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lite\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m converter_error_data_pb2\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lite_constants\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wrap_toco\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvert_phase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Component\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/lite/python/util.py:52\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Jax functions used by TFLite\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m xla_computation \u001b[38;5;28;01mas\u001b[39;00m _xla_computation\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m   _xla_computation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/jax/__init__.py:160\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m abstract_arrays \u001b[38;5;28;01mas\u001b[39;00m _deprecated_abstract_arrays\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m custom_derivatives \u001b[38;5;28;01mas\u001b[39;00m custom_derivatives\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m custom_batching \u001b[38;5;28;01mas\u001b[39;00m custom_batching\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m custom_transpose \u001b[38;5;28;01mas\u001b[39;00m custom_transpose\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api_util \u001b[38;5;28;01mas\u001b[39;00m api_util\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/jax/custom_batching.py:15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2021 The JAX Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcustom_batching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m   custom_vmap,\n\u001b[1;32m     17\u001b[0m   sequential_vmap,\n\u001b[1;32m     18\u001b[0m )\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/jax/_src/custom_batching.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moperator\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, Optional\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lax\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/jax/lax/__init__.py:369\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mann\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    364\u001b[0m   approx_max_k \u001b[38;5;28;01mas\u001b[39;00m approx_max_k,\n\u001b[1;32m    365\u001b[0m   approx_min_k \u001b[38;5;28;01mas\u001b[39;00m approx_min_k,\n\u001b[1;32m    366\u001b[0m   approx_top_k_p \u001b[38;5;28;01mas\u001b[39;00m approx_top_k_p\n\u001b[1;32m    367\u001b[0m )\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mad_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stop_gradient_p \u001b[38;5;28;01mas\u001b[39;00m stop_gradient_p\n\u001b[0;32m--> 369\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg \u001b[38;5;28;01mas\u001b[39;00m linalg\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpjit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m with_sharding_constraint \u001b[38;5;28;01mas\u001b[39;00m with_sharding_constraint\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpjit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sharding_constraint_p \u001b[38;5;28;01mas\u001b[39;00m sharding_constraint_p\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/jax/lax/linalg.py:15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The JAX Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m   cholesky,\n\u001b[1;32m     17\u001b[0m   cholesky_p,\n\u001b[1;32m     18\u001b[0m   eig,\n\u001b[1;32m     19\u001b[0m   eig_p,\n\u001b[1;32m     20\u001b[0m   eigh,\n\u001b[1;32m     21\u001b[0m   eigh_p,\n\u001b[1;32m     22\u001b[0m   hessenberg,\n\u001b[1;32m     23\u001b[0m   hessenberg_p,\n\u001b[1;32m     24\u001b[0m   lu,\n\u001b[1;32m     25\u001b[0m   lu_p,\n\u001b[1;32m     26\u001b[0m   lu_pivots_to_permutation,\n\u001b[1;32m     27\u001b[0m   householder_product,\n\u001b[1;32m     28\u001b[0m   householder_product_p,\n\u001b[1;32m     29\u001b[0m   qr,\n\u001b[1;32m     30\u001b[0m   qr_p,\n\u001b[1;32m     31\u001b[0m   svd,\n\u001b[1;32m     32\u001b[0m   svd_p,\n\u001b[1;32m     33\u001b[0m   triangular_solve,\n\u001b[1;32m     34\u001b[0m   triangular_solve_p,\n\u001b[1;32m     35\u001b[0m   tridiagonal,\n\u001b[1;32m     36\u001b[0m   tridiagonal_p,\n\u001b[1;32m     37\u001b[0m   tridiagonal_solve,\n\u001b[1;32m     38\u001b[0m   tridiagonal_solve_p,\n\u001b[1;32m     39\u001b[0m   schur,\n\u001b[1;32m     40\u001b[0m   schur_p\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqdwh\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     45\u001b[0m   qdwh \u001b[38;5;28;01mas\u001b[39;00m qdwh\n\u001b[1;32m     46\u001b[0m )\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/jax/_src/lax/linalg.py:37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpreters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mlir\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_flow\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m eigh \u001b[38;5;28;01mas\u001b[39;00m lax_eigh\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lax \u001b[38;5;28;01mas\u001b[39;00m lax_internal\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m svd \u001b[38;5;28;01mas\u001b[39;00m lax_svd\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/jax/_src/lax/eigh.py:39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ufuncs\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lax\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m qdwh\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg \u001b[38;5;28;01mas\u001b[39;00m lax_linalg\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Stack\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/jax/_src/lax/qdwh.py:31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional, Tuple\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lax\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/jax/numpy/__init__.py:260\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# TODO(phawkins): make this import unconditional after increasing the ml_dtypes\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# minimum version.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlax_numpy\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_src\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mlax_numpy, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint4\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    261\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlax_numpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    262\u001b[0m     int4 \u001b[38;5;28;01mas\u001b[39;00m int4,\n\u001b[1;32m    263\u001b[0m     uint4 \u001b[38;5;28;01mas\u001b[39;00m uint4,\n\u001b[1;32m    264\u001b[0m   )\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# TODO(jakevdp): make this import unconditional after increasing the minimum\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# version for ml_dtypes and jaxlib\u001b[39;00m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'jax' has no attribute '_src' (most likely due to a circular import)"
          ],
          "ename": "AttributeError",
          "evalue": "partially initialized module 'jax' has no attribute '_src' (most likely due to a circular import)",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Disaster Tweets\n",
        "Let's have a look at the train and test dataset.\n",
        "\n",
        "They contain:\n",
        "- id\n",
        "- keyword: A keyword from that tweet (although this may be blank!)\n",
        "- location: The location the tweet was sent from (may also be blank)\n",
        "- text: The text of a tweet\n",
        "- target: 1 if the tweet is a real disaster or 0 if not"
      ],
      "metadata": {
        "id": "F3-tU-EFiV0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
        "df_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n",
        "\n",
        "print('Training Set Shape = {}'.format(df_train.shape))\n",
        "print('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() / 1024**2))\n",
        "print('Test Set Shape = {}'.format(df_test.shape))\n",
        "print('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() / 1024**2))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-04-09T03:06:13.77217Z",
          "iopub.execute_input": "2025-04-09T03:06:13.772607Z",
          "iopub.status.idle": "2025-04-09T03:06:13.862434Z",
          "shell.execute_reply.started": "2025-04-09T03:06:13.772577Z",
          "shell.execute_reply": "2025-04-09T03:06:13.860848Z"
        },
        "trusted": true,
        "id": "R_ANSkI3iV0P",
        "outputId": "d967d500-755c-4945-ec10-88b48bae0a07"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Training Set Shape = (7613, 5)\nTraining Set Memory Usage = 0.29 MB\nTest Set Shape = (3263, 4)\nTest Set Memory Usage = 0.10 MB\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-04-09T03:06:22.753646Z",
          "iopub.execute_input": "2025-04-09T03:06:22.753998Z",
          "iopub.status.idle": "2025-04-09T03:06:22.774708Z",
          "shell.execute_reply.started": "2025-04-09T03:06:22.753974Z",
          "shell.execute_reply": "2025-04-09T03:06:22.773272Z"
        },
        "trusted": true,
        "id": "M8AkYeGFiV0P",
        "outputId": "b6782b26-4efe-48ea-ee94-c5333400cc36"
      },
      "outputs": [
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-04-09T03:06:25.132982Z",
          "iopub.execute_input": "2025-04-09T03:06:25.1339Z",
          "iopub.status.idle": "2025-04-09T03:06:25.146749Z",
          "shell.execute_reply.started": "2025-04-09T03:06:25.133861Z",
          "shell.execute_reply": "2025-04-09T03:06:25.145354Z"
        },
        "trusted": true,
        "id": "ZX-cI7EiiV0P",
        "outputId": "ee8d6316-26d9-446d-b588-cd15ae0f3882"
      },
      "outputs": [
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": "   id keyword location                                               text\n0   0     NaN      NaN                 Just happened a terrible car crash\n1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just happened a terrible car crash</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Heard about #earthquake is different cities, s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore the dataset"
      ],
      "metadata": {
        "id": "Bafhue6CiV0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[\"length\"] = df_train[\"text\"].apply(lambda x : len(x))\n",
        "df_test[\"length\"] = df_test[\"text\"].apply(lambda x : len(x))\n",
        "\n",
        "print(\"Train Length Stat\")\n",
        "print(df_train[\"length\"].describe())\n",
        "print()\n",
        "\n",
        "print(\"Test Length Stat\")\n",
        "print(df_test[\"length\"].describe())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-04-09T03:05:09.081267Z",
          "iopub.status.idle": "2025-04-09T03:05:09.081958Z",
          "shell.execute_reply.started": "2025-04-09T03:05:09.081646Z",
          "shell.execute_reply": "2025-04-09T03:05:09.081677Z"
        },
        "trusted": true,
        "id": "ovAJobS7iV0P"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to know more information about the data, you can grab useful information [here](https://www.kaggle.com/code/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert)\n",
        "\n",
        "Note that all the tweets are in english."
      ],
      "metadata": {
        "id": "c70KB4mmiV0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess the data"
      ],
      "metadata": {
        "id": "H-CKnJvfiV0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "NUM_TRAINING_EXAMPLES = df_train.shape[0]\n",
        "TRAIN_SPLIT = 0.8\n",
        "VAL_SPLIT = 0.2\n",
        "STEPS_PER_EPOCH = int(NUM_TRAINING_EXAMPLES)*TRAIN_SPLIT // BATCH_SIZE\n",
        "\n",
        "EPOCHS = 2\n",
        "AUTO = tf.data.experimental.AUTOTUNE"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-04-09T03:05:09.083352Z",
          "iopub.status.idle": "2025-04-09T03:05:09.083956Z",
          "shell.execute_reply.started": "2025-04-09T03:05:09.08368Z",
          "shell.execute_reply": "2025-04-09T03:05:09.083707Z"
        },
        "trusted": true,
        "id": "qAS7cUcHiV0Q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_train[\"text\"]\n",
        "y = df_train[\"target\"]\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=VAL_SPLIT, random_state=42)\n",
        "\n",
        "X_test = df_test[\"text\"]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-04-09T03:05:09.086479Z",
          "iopub.status.idle": "2025-04-09T03:05:09.087085Z",
          "shell.execute_reply.started": "2025-04-09T03:05:09.086805Z",
          "shell.execute_reply": "2025-04-09T03:05:09.086831Z"
        },
        "trusted": true,
        "id": "AzcGdo5MiV0Q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load a DistilBERT model from Keras NLP\n",
        "\n",
        "Text inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT.\n",
        "\n",
        "The BertClassifier model can be configured with a preprocessor layer, in which case it will automatically apply preprocessing to raw inputs during fit(), predict(), and evaluate(). This is done by default when creating the model with from_preset().\n",
        "\n",
        "We will choose DistilBERT model.that learns a distilled (approximate) version of BERT, retaining 97% performance but using only half the number of parameters ([paper](https://arxiv.org/abs/1910.01108)).\n",
        "\n",
        "It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT’s performances as measured on the GLUE language understanding benchmark.\n",
        "\n",
        "Specifically, it doesn't have token-type embeddings, pooler and retains only half of the layers from Google's BERT."
      ],
      "metadata": {
        "id": "z2tmVUWPiV0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a DistilBERT model.\n",
        "preset= \"distil_bert_base_en_uncased\"\n",
        "\n",
        "# Use a shorter sequence length.\n",
        "preprocessor = keras_nlp.models.DistilBertPreprocessor.from_preset(preset,\n",
        "                                                                   sequence_length=160,\n",
        "                                                                   name=\"preprocessor_4_tweets\"\n",
        "                                                                  )\n",
        "\n",
        "# Pretrained classifier.\n",
        "classifier = keras_nlp.models.DistilBertClassifier.from_preset(preset,\n",
        "                                                               preprocessor = preprocessor,\n",
        "                                                               num_classes=2)\n",
        "\n",
        "classifier.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-04-09T03:05:09.08904Z",
          "iopub.status.idle": "2025-04-09T03:05:09.08951Z",
          "shell.execute_reply.started": "2025-04-09T03:05:09.089301Z",
          "shell.execute_reply": "2025-04-09T03:05:09.08932Z"
        },
        "trusted": true,
        "id": "eErO5SvliV0Q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train your own model, fine-tuning BERT"
      ],
      "metadata": {
        "id": "3aHAnqm3iV0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile\n",
        "classifier.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), #'binary_crossentropy',\n",
        "    optimizer=keras.optimizers.Adam(1e-5),\n",
        "    metrics= [\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Fit\n",
        "history = classifier.fit(x=X_train,\n",
        "                         y=y_train,\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         epochs=EPOCHS,\n",
        "                         validation_data=(X_val, y_val)\n",
        "                        )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-04-09T03:05:09.091269Z",
          "iopub.status.idle": "2025-04-09T03:05:09.091716Z",
          "shell.execute_reply.started": "2025-04-09T03:05:09.091533Z",
          "shell.execute_reply": "2025-04-09T03:05:09.091553Z"
        },
        "trusted": true,
        "id": "RADAbKD9iV0Q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def displayConfusionMatrix(y_true, y_pred, dataset):\n",
        "    disp = ConfusionMatrixDisplay.from_predictions(\n",
        "        y_true,\n",
        "        np.argmax(y_pred, axis=1),\n",
        "        display_labels=[\"Not Disaster\",\"Disaster\"],\n",
        "        cmap=plt.cm.Blues\n",
        "    )\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, np.argmax(y_pred, axis=1)).ravel()\n",
        "    f1_score = tp / (tp+((fn+fp)/2))\n",
        "\n",
        "    disp.ax_.set_title(\"Confusion Matrix on \" + dataset + \" Dataset -- F1 Score: \" + str(f1_score.round(2)))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-04-09T03:05:09.093646Z",
          "iopub.status.idle": "2025-04-09T03:05:09.094002Z",
          "shell.execute_reply.started": "2025-04-09T03:05:09.093841Z",
          "shell.execute_reply": "2025-04-09T03:05:09.093857Z"
        },
        "trusted": true,
        "id": "7AYrCP_eiV0Q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train = classifier.predict(X_train)\n",
        "\n",
        "displayConfusionMatrix(y_train, y_pred_train, \"Training\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-04-09T03:05:09.095261Z",
          "iopub.status.idle": "2025-04-09T03:05:09.095642Z",
          "shell.execute_reply.started": "2025-04-09T03:05:09.095443Z",
          "shell.execute_reply": "2025-04-09T03:05:09.095487Z"
        },
        "trusted": true,
        "id": "TEl6ubjniV0Q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_val = classifier.predict(X_val)\n",
        "\n",
        "displayConfusionMatrix(y_val, y_pred_val, \"Validation\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-04-09T03:05:09.098162Z",
          "iopub.status.idle": "2025-04-09T03:05:09.098649Z",
          "shell.execute_reply.started": "2025-04-09T03:05:09.098403Z",
          "shell.execute_reply": "2025-04-09T03:05:09.098422Z"
        },
        "trusted": true,
        "id": "AAspYM_liV0R"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate the submission file\n",
        "\n",
        "For each tweets in the test set, we predict if the given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\n",
        "\n",
        "The `submission.csv` file uses the following format:\n",
        "`id,target`"
      ],
      "metadata": {
        "id": "JBncAXW2iV0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n",
        "sample_submission.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-04-09T03:05:09.099876Z",
          "iopub.status.idle": "2025-04-09T03:05:09.100249Z",
          "shell.execute_reply.started": "2025-04-09T03:05:09.100086Z",
          "shell.execute_reply": "2025-04-09T03:05:09.100102Z"
        },
        "trusted": true,
        "id": "61B3WF-fiV0R"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission[\"target\"] = np.argmax(classifier.predict(X_test), axis=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-04-09T03:05:09.10279Z",
          "iopub.status.idle": "2025-04-09T03:05:09.10325Z",
          "shell.execute_reply.started": "2025-04-09T03:05:09.103055Z",
          "shell.execute_reply": "2025-04-09T03:05:09.103075Z"
        },
        "trusted": true,
        "id": "p16i2zyqiV0R"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission.describe()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-04-09T03:05:09.104672Z",
          "iopub.status.idle": "2025-04-09T03:05:09.105104Z",
          "shell.execute_reply.started": "2025-04-09T03:05:09.104916Z",
          "shell.execute_reply": "2025-04-09T03:05:09.104936Z"
        },
        "trusted": true,
        "id": "4kti9TTriV0R"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission.to_csv(\"submission.csv\", index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-04-09T03:05:09.106994Z",
          "iopub.status.idle": "2025-04-09T03:05:09.107481Z",
          "shell.execute_reply.started": "2025-04-09T03:05:09.107251Z",
          "shell.execute_reply": "2025-04-09T03:05:09.10727Z"
        },
        "trusted": true,
        "id": "ZE5ucRTUiV0R"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}